1:"$Sreact.fragment"
2:I[7555,[],""]
3:I[1295,[],""]
4:I[6874,["874","static/chunks/874-437a265a67d6cfee.js","974","static/chunks/app/page-181cdff711d2409c.js"],""]
15:I[8393,[],""]
:HL["/Personal_site/_next/static/css/ef46db3751d8e999.css","style"]
0:{"P":null,"b":"U9pkstdWHB-dyMmr-GNcY","p":"/Personal_site","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/Personal_site/_next/static/css/ef46db3751d8e999.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"font-mono text-sm","children":["$","body",null,{"children":["$","div",null,{"className":"p-4 md:p-12 overflow-hidden","children":["$","div",null,{"className":"w-full max-w-xl","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]}]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[[["$","header",null,{"className":"flex flex-col md:flex-row justify-between items-start md:items-center mb-6 gap-4","children":[["$","$L4",null,{"href":"/","className":"text-3xl font-serif","children":"Physical Intelligence (œÄ)"}],["$","nav",null,{"className":"flex gap-4","children":[["$","$L4",null,{"href":"/","className":"text-link decoration-2 hover:decoration-foreground hover:text-foreground","children":"Home"}],["$","$L4",null,{"href":"/blog","className":"text-link hover:decoration-2 hover:decoration-foreground/50 hover:text-foreground/70","children":"Research"}],["$","$L4",null,{"href":"/join-us","className":"text-link hover:decoration-2 hover:decoration-foreground/50 hover:text-foreground/70","children":"Join Us"}]]}]]}],["$","main",null,{"children":[["$","p",null,{"className":"mt-8 mb-8","children":"Physical Intelligence is bringing general-purpose AI into the physical world. We are a group of engineers, scientists, roboticists, and company builders developing foundation models and learning algorithms to power the robots of today and the physically-actuated devices of the future."}],["$","div",null,{"className":"pl-2","children":["$","div",null,{"className":"relative flex flex-col space-y-4 border-l border-gray-300 py-4 before:h-6 before:w-px before:bg-gradient-to-t before:from-transparent before:to-background before:absolute before:-left-px before:top-0 after:h-6 after:w-px after:bg-gradient-to-b after:from-transparent after:to-background after:absolute after:-left-px after:bottom-0","children":[["$","$L4","0",{"href":"/research/real-time-chunking","className":"flex flex-col gap-1 px-3 py-2 ml-6 group cursor-pointer hover:bg-background-hover","children":[["$","div",null,{"className":"flex items-baseline gap-1 justify-between text-xs w-full relative","children":[["$","button",null,{"className":"absolute size-[7px] bg-gray-900 rounded-sm -left-[40px] top-[4px] outline outline-background outline-2"}],["$","div",null,{"className":"flex items-baseline gap-2 relative grow truncate","children":["$","div",null,{"className":"font-semibold truncate","title":"Real-Time Action Chunking with Large Models","children":"Real-Time Action Chunking with Large Models"}]}],["$","div",null,{"className":"text-xs text-muted-foreground whitespace-nowrap shrink-0","children":"June 9, 2025"}]]}],"$L5"]}],"$L6","$L7","$L8","$L9","$La","$Lb"]}]}],"$Lc","$Ld","$Le","$Lf","$L10","$L11"]}],"$L12"],null,"$L13"]}],{},null,false]},null,false],"$L14",false]],"m":"$undefined","G":["$15",[]],"s":false,"S":true}
16:I[9665,[],"OutletBoundary"]
18:I[4911,[],"AsyncMetadataOutlet"]
1a:I[9665,[],"ViewportBoundary"]
1c:I[9665,[],"MetadataBoundary"]
1d:"$Sreact.suspense"
5:["$","div",null,{"className":"text-xs text-muted-foreground","children":"A real-time system for large VLAs that maintains precision and speed in the face of high latency."}]
6:["$","$L4","1",{"href":"/research/knowledge-insulation","className":"flex flex-col gap-1 px-3 py-2 ml-6 group cursor-pointer hover:bg-background-hover","children":[["$","div",null,{"className":"flex items-baseline gap-1 justify-between text-xs w-full relative","children":[["$","button",null,{"className":"absolute size-[7px] bg-gray-900 rounded-sm -left-[40px] top-[4px] outline outline-background outline-2"}],["$","div",null,{"className":"flex items-baseline gap-2 relative grow truncate","children":["$","div",null,{"className":"font-semibold truncate","title":"VLAs that Train Fast, Run Fast, and Generalize Better","children":"VLAs that Train Fast, Run Fast, and Generalize Better"}]}],["$","div",null,{"className":"text-xs text-muted-foreground whitespace-nowrap shrink-0","children":"May 28, 2025"}]]}],["$","div",null,{"className":"text-xs text-muted-foreground","children":"A method to train vision-language-action models that train quickly, maintain internet-scale knowledge, have high quality inference properties, and generalize well."}]]}]
7:["$","$L4","2",{"href":"/blog/pi05","className":"flex flex-col gap-1 px-3 py-2 ml-6 group cursor-pointer bg-white border border-black shadow-[3px_3px_0px_#000] hover:shadow-[5px_5px_0px_#000] transition-all","children":[["$","div",null,{"className":"flex items-baseline gap-1 justify-between text-xs w-full relative","children":[["$","button",null,{"className":"absolute size-[7px] bg-gray-900 rounded-sm -left-[40px] top-[4px] outline outline-background outline-2"}],["$","div",null,{"className":"flex items-baseline gap-2 relative grow truncate","children":["$","div",null,{"className":"font-semibold truncate","title":"œÄ0.5: a VLA with Open-World Generalization","children":"œÄ0.5: a VLA with Open-World Generalization"}]}],["$","div",null,{"className":"text-xs text-muted-foreground whitespace-nowrap shrink-0","children":"April 22, 2025"}]]}],["$","div",null,{"className":"text-xs text-muted-foreground","children":"Our latest generalist policy, œÄ0.5, extends œÄ0 and enables open-world generalization. Our new model can control a mobile manipulator to clean up an entirely new kitchen or bedroom."}]]}]
8:["$","$L4","3",{"href":"/research/hirobot","className":"flex flex-col gap-1 px-3 py-2 ml-6 group cursor-pointer hover:bg-background-hover","children":[["$","div",null,{"className":"flex items-baseline gap-1 justify-between text-xs w-full relative","children":[["$","button",null,{"className":"absolute size-[7px] bg-gray-900 rounded-sm -left-[40px] top-[4px] outline outline-background outline-2"}],["$","div",null,{"className":"flex items-baseline gap-2 relative grow truncate","children":["$","div",null,{"className":"font-semibold truncate","title":"Teaching Robots to Listen and Think Harder","children":"Teaching Robots to Listen and Think Harder"}]}],["$","div",null,{"className":"text-xs text-muted-foreground whitespace-nowrap shrink-0","children":"February 26, 2025"}]]}],["$","div",null,{"className":"text-xs text-muted-foreground","children":"A method for robots to think through complex tasks step by step, incorporating human-in-the-loop feedback."}]]}]
9:["$","$L4","4",{"href":"/blog/openpi","className":"flex flex-col gap-1 px-3 py-2 ml-6 group cursor-pointer hover:bg-background-hover","children":[["$","div",null,{"className":"flex items-baseline gap-1 justify-between text-xs w-full relative","children":[["$","button",null,{"className":"absolute size-[7px] bg-gray-900 rounded-sm -left-[40px] top-[4px] outline outline-background outline-2"}],["$","div",null,{"className":"flex items-baseline gap-2 relative grow truncate","children":["$","div",null,{"className":"font-semibold truncate","title":"Open Sourcing œÄ0","children":"Open Sourcing œÄ0"}]}],["$","div",null,{"className":"text-xs text-muted-foreground whitespace-nowrap shrink-0","children":"February 4, 2025"}]]}],["$","div",null,{"className":"text-xs text-muted-foreground","children":"We are releasing the weights and code for œÄ0 as well as our new œÄ0-FAST autoregressive model."}]]}]
a:["$","$L4","5",{"href":"/research/fast","className":"flex flex-col gap-1 px-3 py-2 ml-6 group cursor-pointer hover:bg-background-hover","children":[["$","div",null,{"className":"flex items-baseline gap-1 justify-between text-xs w-full relative","children":[["$","button",null,{"className":"absolute size-[7px] bg-gray-900 rounded-sm -left-[40px] top-[4px] outline outline-background outline-2"}],["$","div",null,{"className":"flex items-baseline gap-2 relative grow truncate","children":["$","div",null,{"className":"font-semibold truncate","title":"FAST: Efficient Robot Action Tokenization","children":"FAST: Efficient Robot Action Tokenization"}]}],["$","div",null,{"className":"text-xs text-muted-foreground whitespace-nowrap shrink-0","children":"January 16, 2025"}]]}],["$","div",null,{"className":"text-xs text-muted-foreground","children":"A new robot action tokenizer that allows us to train generalist policies 5x faster than previous models."}]]}]
b:["$","$L4","6",{"href":"/blog/pi0","className":"flex flex-col gap-1 px-3 py-2 ml-6 group cursor-pointer bg-white border border-black shadow-[3px_3px_0px_#000] hover:shadow-[5px_5px_0px_#000] transition-all","children":[["$","div",null,{"className":"flex items-baseline gap-1 justify-between text-xs w-full relative","children":[["$","button",null,{"className":"absolute size-[7px] bg-gray-900 rounded-sm -left-[40px] top-[4px] outline outline-background outline-2"}],["$","div",null,{"className":"flex items-baseline gap-2 relative grow truncate","children":["$","div",null,{"className":"font-semibold truncate","title":"œÄ0: Our First Generalist Policy","children":"œÄ0: Our First Generalist Policy"}]}],["$","div",null,{"className":"text-xs text-muted-foreground whitespace-nowrap shrink-0","children":"October 31, 2024"}]]}],["$","div",null,{"className":"text-xs text-muted-foreground","children":"Our first generalist policy, œÄ0, a prototype model that combines large-scale multi-task and multi-robot data collection with a new network architecture to enable the most capable and dexterous generalist robot policy to date."}]]}]
c:["$","h2",null,{"className":"font-bold mt-8 mb-2","children":"Team"}]
d:["$","div",null,{"className":"grid grid-cols-2 md:grid-cols-3 gap-y-1","children":[["$","div","0",{"children":"Ali Amin"}],["$","div","1",{"children":"Ashwin Balakrishna"}],["$","div","2",{"children":"Kevin Black"}],["$","div","3",{"children":"Noah Brown"}],["$","div","4",{"children":"Danny Driess"}],["$","div","5",{"children":"James Darpinian"}],["$","div","6",{"children":"Karan Dhabalia"}],["$","div","7",{"children":"Adnan Esmail"}],["$","div","8",{"children":"Michael Equi"}],["$","div","9",{"children":"Chelsea Finn"}],["$","div","10",{"children":"Nick Fusai"}],["$","div","11",{"children":"Catherine Glossop"}],["$","div","12",{"children":"Lachy Groom"}],["$","div","13",{"children":"Karol Hausman"}],["$","div","14",{"children":"Joey Hejna"}],["$","div","15",{"children":"Gashon Hussein"}],["$","div","16",{"children":"Brian Ichter"}],["$","div","17",{"children":"Szymon Jakubczak"}],["$","div","18",{"children":"Rowan Jen"}],["$","div","19",{"children":"Tim Jones"}],["$","div","20",{"children":"Simar Kareer"}],["$","div","21",{"children":"Ben Katz"}],["$","div","22",{"children":"Kay Ke"}],["$","div","23",{"children":"Marinda Lamb"}],["$","div","24",{"children":"Sergey Levine"}],["$","div","25",{"children":"Adrian Li-Bell"}],["$","div","26",{"children":"Mohith Mothukuri"}],["$","div","27",{"children":"Suraj Nair"}],["$","div","28",{"children":"Karl Pertsch"}],["$","div","29",{"children":"Allen Ren"}],["$","div","30",{"children":"Charvi Sharma"}],["$","div","31",{"children":"Lucy Shi"}],["$","div","32",{"children":"Laura Smith"}],["$","div","33",{"children":"Tobi Springenberg"}],["$","div","34",{"children":"Kyle Stachowicz"}],["$","div","35",{"children":"Alexander Swerdlow"}],["$","div","36",{"children":"James Tanner"}],["$","div","37",{"children":"Marcel Torne"}],["$","div","38",{"children":"Quan Vuong"}],["$","div","39",{"children":"Anna Walling"}],["$","div","40",{"children":"Homer Walke"}],["$","div","41",{"children":"Haohuan Wang"}],["$","div","42",{"children":"Lili Yu"}],["$","div","43",{"children":"Ury Zhilinsky"}],["$","div","44",{"children":"Paul Zhiyuan Zhou"}],["$","div","45",{"children":"...and growing!"}]]}]
e:["$","p",null,{"className":"my-4","children":["If you are interested in joining, please"," ",["$","$L4",null,{"href":"/join-us","className":"text-link hover:decoration-2","children":"get in touch"}],"."]}]
f:["$","h2",null,{"className":"font-bold mt-8 mb-2","children":"Investors"}]
10:["$","p",null,{"children":["We are grateful for"," ",["$","a",null,{"href":"https://www.nytimes.com/2024/11/04/business/dealbook/physical-intelligence-robot-ai.html","className":"text-link hover:decoration-2","target":"_blank","rel":"noopener noreferrer","children":"the support"}]," ","of Bond, Jeff Bezos, Khosla Ventures, Lux Capital, OpenAI, Redpoint Ventures, Sequoia Capital, and Thrive Capital."]}]
11:["$","p",null,{"className":"my-4","children":["You can follow us on ùïè/Twitter at"," ",["$","a",null,{"href":"https://twitter.com/physical_int","className":"text-link hover:decoration-2","target":"_blank","rel":"noopener noreferrer","children":"@physical_int"}]]}]
12:["$","footer",null,{"className":"mt-12","children":["$","div",null,{"className":"max-w-3xl w-full mx-auto flex justify-between py-6 items-baseline border-t border-black","children":[["$","p",null,{"className":"font-serif text-lg","children":"Physical Intelligence"}],["$","div",null,{"className":"flex gap-4"}]]}]}]
13:["$","$L16",null,{"children":["$L17",["$","$L18",null,{"promise":"$@19"}]]}]
14:["$","$1","h",{"children":[null,[["$","$L1a",null,{"children":"$L1b"}],null],["$","$L1c",null,{"children":["$","div",null,{"hidden":true,"children":["$","$1d",null,{"fallback":null,"children":"$L1e"}]}]}]]}]
1b:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
17:null
1f:I[8175,[],"IconMark"]
19:{"metadata":[["$","title","0",{"children":"Physical Intelligence (œÄ)"}],["$","meta","1",{"name":"description","content":"Physical Intelligence is bringing general-purpose AI into the physical world."}],["$","meta","2",{"name":"author","content":"Physical Intelligence"}],["$","meta","3",{"name":"keywords","content":"AI,Robotics,Physical Intelligence,Machine Learning"}],["$","meta","4",{"property":"og:title","content":"Physical Intelligence (œÄ)"}],["$","meta","5",{"property":"og:description","content":"Physical Intelligence is bringing general-purpose AI into the physical world."}],["$","meta","6",{"property":"og:locale","content":"en_US"}],["$","meta","7",{"property":"og:type","content":"website"}],["$","meta","8",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","9",{"name":"twitter:title","content":"Physical Intelligence (œÄ)"}],["$","meta","10",{"name":"twitter:description","content":"Physical Intelligence is bringing general-purpose AI into the physical world."}],["$","link","11",{"rel":"icon","href":"/Personal_site/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","$L1f","12",{}]],"error":null,"digest":"$undefined"}
1e:"$19:metadata"
